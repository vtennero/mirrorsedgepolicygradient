\documentclass[11pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{booktabs}
\usepackage{tabularx}
\usepackage{listings}
\usepackage{xcolor}

\lstset{
    language=CSharp,
    basicstyle=\ttfamily\small,
    keywordstyle=\color{blue},
    commentstyle=\color{green},
    stringstyle=\color{red},
    numbers=left,
    numberstyle=\tiny,
    breaklines=true,
    frame=single
}

\title{Optimizing Rational and Aesthetic Navigation Objectives via Stochastic Reward Shaping in Procedural 3D Unity Environments}
\author{Victor Tenneroni}
\date{}

\begin{document}
\maketitle

\section{Introduction}

\subsection{Problem Statement}
This project addresses autonomous navigation across procedurally generated parkour environments where agents must balance multiple competing objectives: speed (reaching targets efficiently), energy management (stamina conservation), and aesthetic quality (stylistic movement). The core challenge extends beyond standard navigation: the agent should not only reach the target but do so with human-preferred behaviors like dynamic rolls and varied movement patterns.

\subsection{The Human Feedback Challenge}
The ideal approach would employ Reinforcement Learning from Human Feedback (RLHF), where humans directly label preferred trajectories. However, this methodology faces fundamental incompatibility with our training infrastructure: 28 parallel agents operating at 20× time acceleration generate ~1,054 steps/second, producing approximately 30 complete episodes per minute. Real-time human evaluation at this scale is infeasible, creating a critical gap between optimal methods and practical constraints.

\subsection{Contribution: Stochastic Reward Shaping}
We propose episodic stochastic reward modulation as a practical approximation of preference diversity. Rather than uniform reward signals, we inject randomness at the episode level: 40\% of training episodes provide enhanced rewards (+1.5 bonus) for high-cost stylistic actions (rolls), while the remaining 60\% offer only base rewards (+0.5). This approach attempts to model the variance in human aesthetic preferences without requiring real-time feedback.

Key insight: By creating episodes where certain behaviors are disproportionately rewarded, we force the agent to learn those behaviors remain viable strategies, preventing complete dismissal of high-cost actions that humans would find preferable.

\subsection{Empirical Validation}
Across multiple training configurations (2M steps each), we observe:
\begin{itemize}
    \item Baseline without style rewards: 0.69\% roll usage
    \item Stochastic reward shaping (40\% bonus episodes): significantly increased roll usage
    \item Final performance: +89.18 average reward, 632+ units traveled (550\% beyond minimum target)
\end{itemize}

\section{Methodology}

\subsection{Training Infrastructure}

\subsubsection{Parallel Agent Setup}
The training infrastructure employs \textbf{28 parallel agents} running simultaneously across 28 independent \texttt{TrainingArea} objects within a single Unity environment instance. This parallelization strategy enables efficient data collection and significantly accelerates the training process.

\textbf{Key Configuration:}
\begin{itemize}
    \item \textbf{Number of Agents:} 28 (one agent per \texttt{TrainingArea})
    \item \textbf{Number of Environments:} 1 (single Unity instance)
    \item \textbf{Training Areas:} 28 \texttt{TrainingArea} objects in the scene
    \item \textbf{Time Scale:} 20× acceleration multiplier during training
\end{itemize}

\textbf{Design Rationale:}
The choice of 28 agents represents a balance between:
\begin{enumerate}
    \item \textbf{Data Collection Efficiency:} More agents provide more diverse experiences per unit time
    \item \textbf{Computational Overhead:} Each agent requires physics simulation and observation collection
    \item \textbf{Memory Constraints:} Unity scene complexity increases with more agents
    \item \textbf{Practical Limits:} 28 agents was empirically determined to be the maximum stable configuration on the target hardware
\end{enumerate}

\subsubsection{Training Duration and Scale}
\textbf{Training Scale:}
\begin{itemize}
    \item \textbf{Total Training Steps:} 2,000,000 steps
    \item \textbf{Wall-Clock Time:} ~30--32 minutes (approximately 31.6 minutes for complete training)
    \item \textbf{Time Horizon:} 128 steps before value bootstrapping
    \item \textbf{Checkpoint Interval:} Every 500,000 steps
\end{itemize}

\textbf{Training Efficiency Calculation:}
\begin{itemize}
    \item \textbf{Single Agent:} ~8+ hours for 2M steps (at 20× time scale)
    \item \textbf{28 Parallel Agents:} ~30 minutes for 2M steps
    \item \textbf{Speedup Factor:} ~16× improvement (28 agents × 20× time scale = 560× theoretical, but limited by Python training loop and network updates)
\end{itemize}

\subsubsection{Time Acceleration and Offline Preference Modeling}
\textbf{Time Acceleration Necessity:}
The 20× time acceleration factor is essential for practical training durations. However, this acceleration creates a fundamental constraint: \textbf{human feedback is incompatible with accelerated training}. Real-time human evaluation of agent behavior becomes infeasible when the environment runs 20× faster than normal speed.

\textbf{The Fundamental Constraint:}
\begin{itemize}
    \item \textbf{Normal Speed:} Human can evaluate behavior in real-time
    \item \textbf{20× Accelerated:} Environment runs too fast for human perception and reaction
    \item \textbf{Implication:} Cannot use RLHF (Reinforcement Learning from Human Feedback) during training
    \item \textbf{Solution:} Design reward functions that approximate human preferences \textit{a priori}
\end{itemize}

\textbf{Implication for Reward Design:}
This constraint necessitates an \textbf{offline preference modeling approach}. Rather than collecting real-time human feedback during training (as in RLHF), we must design reward functions that approximate human preferences \textit{a priori}. The style reward system (Section 3.2) represents our approach to approximating human aesthetic preferences without requiring real-time human interaction.

\subsubsection{PPO Algorithm and Training Strategy}
\textbf{Algorithm:} Proximal Policy Optimization (PPO)

The training uses PPO with high exploration (beta = 0.1) and linear decay schedules to gradually shift from exploration to exploitation. The network architecture consists of separate actor (2×256) and critic (2×128) networks with input normalization. Detailed hyperparameters and implementation specifics are provided in Section 4.2.

\subsection{Reward Design}

\subsubsection{Design Philosophy and Workflow}
\textbf{Design Philosophy:}
The reward function must guide the agent toward both functional parkour (reaching targets efficiently) and aesthetic parkour (stylish movements). This dual objective creates a multi-objective optimization problem that requires careful reward shaping.

\textbf{Key Design Principles:}
\begin{itemize}
    \item \textbf{Dense Rewards:} Provide learning signal at every step (progress, grounded)
    \item \textbf{Sparse Rewards:} Provide clear success/failure signals (target reach, fall)
    \item \textbf{Shaped Rewards:} Guide agent toward desired behaviors (style bonuses)
    \item \textbf{Magnitude Relationships:} Ensure rewards are properly scaled relative to each other
\end{itemize}

\subsubsection{Multi-Objective Reward Structure}
The reward function combines multiple objectives to guide the agent toward both functional and aesthetic parkour behavior:

\begin{enumerate}
    \item \textbf{Progress Maximization} (Primary objective) -- 79\% of total reward
    \item \textbf{Time Minimization} (Secondary objective) -- Encourages speed
    \item \textbf{Stamina Management} (Tertiary objective) -- Encourages efficiency
    \item \textbf{Style Actions} (Episodic bonus) -- Encourages aesthetic behavior
\end{enumerate}

\subsubsection{Base Rewards: Design and Calibration}
\textbf{Dense Rewards (Per-Step):}

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{lXcX}
\toprule
\textbf{Reward Component} & \textbf{Value} & \textbf{Condition} & \textbf{Design Rationale} \\
\midrule
Progress Reward & $+0.1 \times \Delta x$ & Forward movement & Primary learning signal. 0.1/unit chosen to provide strong gradient while maintaining scale. \\
Grounded Reward & $+0.001$ & Agent is grounded & Encourages staying on platforms. Small magnitude (0.1\% of progress) prevents over-prioritization. \\
Time Penalty & $-0.001$ & Per fixed update & Encourages speed. Magnitude matches grounded reward to balance. \\
Low Stamina Penalty & $-0.002$ & Stamina $< 20\%$ & Discourages keeping stamina at zero. 2× time penalty to emphasize importance. \\
\bottomrule
\end{tabularx}
\caption{Dense reward components}
\end{table}

\textbf{Sparse Rewards (Episode-Level):}

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{lXcX}
\toprule
\textbf{Reward Component} & \textbf{Value} & \textbf{Condition} & \textbf{Design Rationale} \\
\midrule
Target Reach & $+10.0$ & Distance $< 2.0$ units & Clear success signal. Equivalent to 100 units of progress. \\
Fall Penalty & $-1.0$ & Agent falls or timeout & Clear failure signal. Magnitude chosen to be significant but not overwhelming (10\% of target reach). \\
\bottomrule
\end{tabularx}
\caption{Sparse reward components}
\end{table}

\subsubsection{Reward Scaling and Context}
\textbf{Typical Episode Reward Breakdown:}
For a successful episode reaching the target (~700 units of progress):
\begin{itemize}
    \item \textbf{Progress Reward:} ~70.0 (79\% of total) -- $700 \times 0.1 = +70.0$
    \item \textbf{Target Reach:} +10.0 (11\% of total)
    \item \textbf{Grounded Reward:} ~0.85 (1\% of total) -- $850 \times 0.001 = +0.85$
    \item \textbf{Time Penalty:} ~-0.85 (-1\% of total) -- $850 \times -0.001 = -0.85$
    \item \textbf{Roll Rewards:} Variable (~2.2\% per roll) -- $+0.5$ base $+ 1.5$ style $= +2.0$ per roll in style episodes
    \item \textbf{Total Episode Reward:} ~89 (typical successful episode)
\end{itemize}

\subsubsection{Style Reward Approximation: Design Process}
\textbf{Stochastic Reward Shaping as Preference Approximation:}

The style reward system approximates human aesthetic preferences through \textbf{stochastic reward injection} rather than real-time human feedback. This design addresses the fundamental constraint that human feedback is incompatible with accelerated training (Section 3.1).

\textbf{Roll Reward Structure:}

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{lXcX}
\toprule
\textbf{Reward Component} & \textbf{Value} & \textbf{Condition} & \textbf{Design Rationale} \\
\midrule
Roll Base Reward & $+0.5$ & Roll action executed & Ensures rolls are always valuable. Prevents agent from ignoring rolls in non-style episodes. \\
Roll Style Bonus & $+1.5$ & Roll in style episode & Provides additional incentive in 40\% of episodes. Creates behavioral variety. \\
\bottomrule
\end{tabularx}
\caption{Style reward components}
\end{table}

\textbf{Total Roll Reward:}
\begin{itemize}
    \item \textbf{In style episodes (40\%):} $+0.5$ base $+ 1.5$ style $= +2.0$ per roll (20× progress per unit)
    \item \textbf{In non-style episodes (60\%):} $+0.5$ base per roll (5× progress per unit)
\end{itemize}

\textbf{Episode-Level Style Flag:}
\begin{itemize}
    \item \textbf{Probability:} 40\% (\texttt{styleEpisodeFrequency = 0.4})
    \item \textbf{Assignment:} Randomly determined at episode start
    \item \textbf{Scope:} Affects all roll actions within that episode
    \item \textbf{Rationale:} Stochastic injection mimics preference diversity across different human evaluators
\end{itemize}

\subsubsection{Rationale: Stochastic Injection Mimics Preference Diversity}
\textbf{Why Stochastic Episode-Level Flags?}

The episode-level style flag approximates preference diversity across different human evaluators. Rather than a fixed reward structure, the stochastic assignment (40\% probability) creates behavioral variety that mimics how different humans might value style vs. efficiency differently.

\textbf{Why Base Reward + Style Bonus?}
\begin{itemize}
    \item \textbf{Base reward (always given):} Ensures rolls are always valuable, not just in style episodes. This prevents the agent from completely ignoring rolls in non-style episodes.
    \item \textbf{Style bonus (conditional):} Provides additional incentive in 40\% of episodes, creating behavioral variety and encouraging occasional stylish movement.
\end{itemize}

\subsection{State/Action Space}

\subsubsection{State Space Design Philosophy}
\textbf{Design Goals:}
\begin{enumerate}
    \item \textbf{Sufficient Information:} Agent must have enough information to make good decisions
    \item \textbf{Minimal Dimensionality:} Smaller state space = faster learning
    \item \textbf{Generalization:} State space must work across different platform layouts
    \item \textbf{Interpretability:} State components should have clear semantic meaning
\end{enumerate}

\subsubsection{State Space (Observations)}
\textbf{Total Observations: 14 floats}

The state space is fully observable and consists of the following components:

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{lXcX}
\toprule
\textbf{Observation Component} & \textbf{Size} & \textbf{Description} & \textbf{Range/Normalization} \\
\midrule
Target Relative Position & 3 floats & $(target.position - agent.position)$ & Raw 3D vector (units) \\
Velocity & 3 floats & $controller.velocity$ & Raw 3D vector (units/sec) \\
Grounded State & 1 float & $1.0$ if grounded, $0.0$ if not & Binary (0.0 or 1.0) \\
Platform Raycasts & 5 floats & Downward raycasts at [2, 4, 6, 8, 10] units ahead & Normalized (0.0--1.0) \\
Obstacle Distance & 1 float & Forward obstacle raycast distance & Normalized (0.0--1.0) \\
Stamina & 1 float & $currentStamina / maxStamina$ & Normalized (0.0--1.0) \\
\bottomrule
\end{tabularx}
\caption{State space components}
\end{table}

\textbf{State Space Properties:}
\begin{itemize}
    \item \textbf{Dimensionality:} 14 ($S \subseteq \mathbb{R}^{14}$)
    \item \textbf{Observability:} Fully observable (no hidden information)
    \item \textbf{Normalization:} Applied where applicable (raycasts, stamina)
    \item \textbf{Completeness:} Contains all information needed for parkour decisions
\end{itemize}

\subsubsection{Platform Detection Raycasts: Critical Design Decision}
\textbf{Purpose:} Detect gaps and platform edges ahead of the agent to enable gap detection and jump timing.

\textbf{Implementation Details:}
\begin{itemize}
    \item \textbf{5 downward raycasts} at forward distances: $[2, 4, 6, 8, 10]$ units ahead
    \item \textbf{Ray origin:} $agent.position + forward \times distance + Vector3.up \times 0.5$
    \item \textbf{Ray direction:} $Vector3.down$
    \item \textbf{Max ray distance:} $10$ (normalization factor)
    \item \textbf{Output encoding:}
    \begin{itemize}
        \item Platform detected: $hit.distance / maxRayDist$ (0.0--1.0, where 0.0 = platform at ray origin)
        \item No platform (gap): $1.0$ (normalized max distance)
    \end{itemize}
\end{itemize}

\textbf{Critical Design: Perception for Generalization}

\textbf{Empirical Evidence:}
\begin{itemize}
    \item \textbf{Experiment:} test\_v9 (no raycasts) vs. test\_v10 (5 raycasts) in randomized environment
    \item \textbf{Result:} +3.43 vs. +9.85 reward (187\% improvement)
    \item \textbf{Interpretation:} Without raycasts, agent cannot adapt to randomized gap spacing (2.5--4.5 units)
    \item \textbf{Conclusion:} Platform raycasts are \textbf{essential} for generalization to randomized environments
\end{itemize}

\subsubsection{Action Space Design}
\textbf{Type:} Discrete, single branch, 5 actions

\begin{table}[h]
\centering
\small
\begin{tabularx}{\textwidth}{lXcX}
\toprule
\textbf{Action} & \textbf{ID} & \textbf{Description} & \textbf{Constraints} \\
\midrule
Idle & 0 & No movement & Always available \\
Jump & 1 & Vertical jump with forward boost & Requires: $isGrounded \land stamina \geq 20.0$ \\
Jog & 2 & Forward movement at 6 units/sec & Always available \\
Sprint & 3 & Forward movement at 12 units/sec & Requires: $stamina > 0 \land \neg cooldown$ \\
Roll & 4 & Forward roll at 18 units/sec & Requires: $stamina \geq 60.0 \land \neg isRolling$ \\
\bottomrule
\end{tabularx}
\caption{Action space}
\end{table}

\textbf{Action Space Properties:}
\begin{itemize}
    \item \textbf{Type:} Discrete ($A = \{0, 1, 2, 3, 4\}$)
    \item \textbf{Branch Count:} 1 (single decision branch)
    \item \textbf{Action Count:} 5
    \item \textbf{Constraints:} Enforced by environment (stamina, cooldown, grounded state)
\end{itemize}

\section{Implementation}

\subsection{Unity ML-Agents Setup}

\subsubsection{Environment Configuration}
The training environment is built using \textbf{Unity 2022.3 LTS} with the \textbf{ML-Agents Toolkit (version 1.1.0)}. The implementation follows the standard ML-Agents architecture with custom extensions for parkour-specific behaviors.

\textbf{Core Components:}
\begin{itemize}
    \item \textbf{Agent Script:} \texttt{ParkourAgent.cs} -- Inherits from \texttt{Unity.MLAgents.Agent}
    \item \textbf{Training Areas:} 28 \texttt{TrainingArea} objects in the scene (one per parallel agent)
    \item \textbf{Character Controller:} Unity's built-in \texttt{CharacterController} component for physics-based movement
    \item \textbf{Configuration System:} \texttt{CharacterConfig} ScriptableObject for centralized parameter management
\end{itemize}

\textbf{ML-Agents Integration:}
\begin{itemize}
    \item \textbf{Package Version:} \texttt{com.unity.ml-agents} 3.0.0+ (Unity Package Manager)
    \item \textbf{Python Package:} \texttt{mlagents} 1.1.0 (via conda/pip)
    \item \textbf{Communication:} Unity $\leftrightarrow$ Python via gRPC on port 5004 (default)
    \item \textbf{Behavior Name:} \texttt{ParkourRunner} (must match in config and Unity)
\end{itemize}

\subsubsection{Training Workflow: Detailed Process}
\textbf{Training Command:}
\begin{lstlisting}[language=bash]
cd src
conda activate mlagents
python train_with_progress.py parkour_config.yaml --run-id=training_<timestamp> --force
\end{lstlisting}

\textbf{Step-by-Step Training Process:}

\textbf{Phase 1: Initialization (0--5 seconds)}
\begin{enumerate}
    \item \textbf{Python Script Execution:}
    \begin{itemize}
        \item \texttt{train\_with\_progress.py} reads \texttt{parkour\_config.yaml}
        \item Parses max\_steps (2,000,000) and behavior name (\texttt{ParkourRunner})
        \item Auto-generates run-id: \texttt{training\_YYYYMMDD\_HHMMSS}
        \item Launches \texttt{mlagents-learn} subprocess with config file
    \end{itemize}
    \item \textbf{ML-Agents Trainer Startup:}
    \begin{itemize}
        \item Python trainer initializes PyTorch model (actor + critic networks)
        \item Opens gRPC server on port 5004
        \item Waits for Unity connection
    \end{itemize}
    \item \textbf{Unity Editor Connection:}
    \begin{itemize}
        \item User opens \texttt{TrainingScene.unity} in Unity Editor
        \item Scene contains 28 \texttt{TrainingArea} objects, each with a \texttt{ParkourAgent}
        \item User presses \textbf{Play} button in Unity Editor
        \item Unity ML-Agents connects to Python trainer on port 5004
    \end{itemize}
\end{enumerate}

\textbf{Phase 2: Training Loop (30 minutes)}
\begin{enumerate}
    \setcounter{enumi}{3}
    \item \textbf{Experience Collection:}
    \begin{itemize}
        \item Each of 28 agents collects experiences simultaneously
        \item At 50Hz (20ms per step), each agent:
        \begin{itemize}
            \item Collects observations (14 floats)
            \item Receives action from policy network
            \item Executes action (with constraints)
            \item Calculates rewards
            \item Stores experience tuple: $(state, action, reward, next\_state)$
        \end{itemize}
    \end{itemize}
    \item \textbf{Batch Processing:}
    \begin{itemize}
        \item When buffer reaches \texttt{time\_horizon} (128 steps) × 28 agents = 3,584 experiences:
        \begin{itemize}
            \item Trainer samples \texttt{batch\_size} (1024) experiences
            \item Computes advantages using GAE ($\lambda=0.95$, $\gamma=0.99$)
            \item Trains policy network for \texttt{num\_epoch} (5) epochs
            \item Updates value network (critic)
            \item Clears buffer, continues collection
        \end{itemize}
    \end{itemize}
    \item \textbf{Progress Tracking:}
    \begin{itemize}
        \item \texttt{train\_with\_progress.py} intercepts ML-Agents output
        \item Parses step count from log lines: \texttt{[INFO] ParkourRunner. Step: 680000.}
        \item Calculates percentage: $(current\_steps / max\_steps) \times 100$
        \item Displays: \texttt{[34.0\%] Time Elapsed: 735.333 s. Mean Reward: 9.899.}
    \end{itemize}
    \item \textbf{Checkpointing:}
    \begin{itemize}
        \item Every 500,000 steps: Model saved to \texttt{src/results/training\_*/ParkourRunner.onnx}
        \item Summary logs saved to \texttt{src/results/training\_*/run\_logs/}
        \item TensorBoard logs updated for visualization
    \end{itemize}
\end{enumerate}

\textbf{Phase 3: Completion (2M steps)}
\begin{enumerate}
    \setcounter{enumi}{7}
    \item \textbf{Training Completion:}
    \begin{itemize}
        \item Final model saved at 2,000,000 steps
        \item Training statistics written to \texttt{timers.json} and \texttt{training\_status.json}
        \item Python script exits
        \item Unity Editor can be stopped
    \end{itemize}
\end{enumerate}

\subsection{Training Hyperparameters}

\subsubsection{PPO Configuration}
The training uses Proximal Policy Optimization (PPO) with the following hyperparameters defined in \texttt{parkour\_config.yaml}:

\textbf{Hyperparameters:}
\begin{itemize}
    \item \textbf{Learning Rate:} $3.0 \times 10^{-4}$ (linear decay schedule)
    \item \textbf{Batch Size:} 1024 experiences per training batch
    \item \textbf{Buffer Size:} 10240 (10× batch size for experience replay)
    \item \textbf{Beta (Entropy):} 0.1 (linear decay) -- High exploration coefficient
    \item \textbf{Epsilon (Clipping):} 0.2 (linear decay) -- PPO clipping parameter
    \item \textbf{Lambda (GAE):} 0.95 -- Generalized Advantage Estimation lambda
    \item \textbf{Gamma (Discount):} 0.99 -- Discount factor for future rewards
    \item \textbf{Num Epochs:} 5 -- Training epochs per batch
    \item \textbf{Time Horizon:} 128 steps before value bootstrapping
\end{itemize}

\textbf{Network Architecture:}
\begin{itemize}
    \item \textbf{Actor Network:} 2 hidden layers × 256 units, input normalization enabled
    \item \textbf{Critic Network:} 2 hidden layers × 128 units, separate from actor (not shared)
    \item \textbf{Activation:} ReLU (default ML-Agents)
    \item \textbf{Initialization:} Xavier/Glorot uniform (ML-Agents default)
\end{itemize}

\subsubsection{Hyperparameter Selection Rationale}
\textbf{High Beta (0.1):} Increased from default 0.015 to encourage exploration in the complex parkour environment. The linear decay schedule allows gradual shift from exploration to exploitation.

\textbf{Selection Process:}
\begin{itemize}
    \item \textbf{Initial Value:} 0.015 (ML-Agents default)
    \item \textbf{Problem:} Agent converged too quickly, missed optimal strategies
    \item \textbf{Experimentation:} Tested 0.05, 0.1, 0.2
    \item \textbf{Result:} 0.1 provided best balance (high exploration, still learns effectively)
    \item \textbf{Decay:} Linear from 0.1 → ~0.00074 over 2M steps
\end{itemize}

\textbf{Time Horizon 128:} Balanced between shorter horizons (64) that may miss long-term dependencies and longer horizons (192) that slow training. Appropriate for 100-second episodes.

\textbf{Decay Formulas:}
\begin{align}
lr(t) &= 3.0 \times 10^{-4} \times \left(1 - \frac{t}{2,000,000}\right) \\
beta(t) &= 0.1 \times \left(1 - \frac{t}{2,000,000}\right) \\
epsilon(t) &= 0.2 \times \left(1 - \frac{t}{2,000,000}\right)
\end{align}

\subsection{Style Episode Frequency: Why 40\%?}

\subsubsection{Empirical Evolution}
The style episode frequency was \textbf{increased from 15\% to 40\%} during development based on empirical observations:

\textbf{Initial Design (15\%):}
\begin{itemize}
    \item Original implementation used \texttt{styleEpisodeFrequency = 0.15}
    \item Rationale: Provide occasional style incentives without overwhelming functional objectives
    \item Result: Roll usage remained low (~0.69\% of actions, 28.1 rolls/episode)
    \item \textbf{Training Run:} \texttt{training\_20251207\_171550} (previous run before frequency increase)
\end{itemize}

\textbf{Current Design (40\%):}
\begin{itemize}
    \item Increased to \texttt{styleEpisodeFrequency = 0.4} (40\% of episodes)
    \item Rationale: Provide more opportunities for style actions to be learned and expressed
    \item Result: Significantly increased roll usage (exact percentage from training logs)
    \item \textbf{Training Run:} \texttt{training\_20251207\_210205} (current run with 40\% frequency)
\end{itemize}

\textbf{Empirical Evidence:}
\begin{itemize}
    \item \textbf{15\% Frequency:} Roll usage ~0.69\% of actions, agent rarely used rolls
    \item \textbf{40\% Frequency:} Roll usage significantly increased (user confirmed ``more rolls'')
    \item \textbf{Reward Improvement:} +31\% improvement (+67.90 → +89.18) with 40\% frequency
\end{itemize}

\subsubsection{Selection Criteria}
The 40\% frequency was chosen to balance three competing objectives:

\begin{enumerate}
    \item \textbf{Sufficient Style Incentive:} High enough frequency to ensure the agent learns roll usage patterns
    \item \textbf{Functional Behavior Preservation:} Low enough that 60\% of episodes focus purely on functional objectives (progress, speed, efficiency)
    \item \textbf{Behavioral Variety:} Creates diversity in agent behavior across episodes
\end{enumerate}

\subsubsection{Acknowledgment of Arbitrariness}
\textbf{We acknowledge that the 40\% value is somewhat arbitrary} and was selected through empirical tuning rather than theoretical optimization. The choice represents a practical balance point that:
\begin{itemize}
    \item Provides sufficient style signal for learning
    \item Maintains functional behavior in majority of episodes
    \item Approximates preference diversity (different human evaluators might prefer different style/efficiency trade-offs)
\end{itemize}

\textbf{Alternative Frequencies Considered:}
\begin{itemize}
    \item \textbf{10--20\%:} Too infrequent, agent rarely learns style actions (observed in 15\% experiment)
    \item \textbf{50--60\%:} Too frequent, risks prioritizing style over function (not tested, but theoretical concern)
    \item \textbf{40\%:} Empirical sweet spot observed in training experiments
\end{itemize}

\textbf{Theoretical Justification (Post-Hoc):}
While 40\% was chosen empirically, we can justify it post-hoc:
\begin{itemize}
    \item \textbf{Majority Functional (60\%):} Ensures agent prioritizes reaching target
    \item \textbf{Substantial Style (40\%):} Provides enough style signal for learning
    \item \textbf{Preference Diversity:} Mimics scenario where 40\% of human evaluators prefer style, 60\% prefer efficiency
\end{itemize}

\subsubsection{Implementation Details}
\textbf{Code Location:} \texttt{src/Assets/Scripts/CharacterConfig.cs}
\begin{lstlisting}
[Tooltip("Probability that an episode will have style bonuses enabled (0.1 = 10%, 0.2 = 20%)")]
[Range(0f, 1f)]
public float styleEpisodeFrequency = 0.4f; // Increased from 15% to 40%
\end{lstlisting}

\textbf{Assignment Logic:} \texttt{src/Assets/Scripts/ParkourAgent.cs}
\begin{lstlisting}
public override void OnEpisodeBegin()
{
    // ... other reset logic ...
    
    // Randomly assign style bonus flag at episode start
    styleBonusEnabled = Random.Range(0f, 1f) < config.styleEpisodeFrequency;
    
    // ... rest of reset logic ...
}
\end{lstlisting}

\textbf{Impact:} The flag is assigned once per episode and affects all roll actions within that episode. This episodic-level assignment ensures consistent reward structure throughout each episode, making it easier for the agent to learn the relationship between style episodes and roll rewards.

\subsection{Configuration System}

\subsubsection{Dual Configuration Architecture}
The implementation uses a \textbf{dual configuration system} to separate training hyperparameters from environment/gameplay parameters:

\textbf{1. ML-Agents Config (\texttt{parkour\_config.yaml}):}
\begin{itemize}
    \item PPO hyperparameters (learning rate, batch size, etc.)
    \item Network architecture settings
    \item Training schedule (max steps, checkpoints)
    \item \textbf{Location:} \texttt{src/parkour\_config.yaml}
    \item \textbf{Format:} YAML
    \item \textbf{Edited:} Text editor (no Unity required)
\end{itemize}

\textbf{2. Unity ScriptableObject (\texttt{CharacterConfig.cs}):}
\begin{itemize}
    \item Movement parameters (speeds, jump force, gravity)
    \item Stamina system (max, consumption, regen rates)
    \item Reward values (progress multiplier, target reach, roll rewards)
    \item Environment settings (episode timeout, raycast distances)
    \item Style system (roll base reward, style bonus, frequency)
    \item \textbf{Location:} Unity Project (\texttt{Assets/Settings/CharacterConfig.asset})
    \item \textbf{Format:} Unity ScriptableObject (serialized as \texttt{.asset} file)
    \item \textbf{Edited:} Unity Inspector (visual editor)
\end{itemize}

\textbf{Rationale:} This separation allows:
\begin{itemize}
    \item \textbf{Training hyperparameters} to be adjusted without Unity recompilation
    \item \textbf{Gameplay parameters} to be tuned in Unity Editor with immediate visual feedback
    \item \textbf{Version control} of both configuration types independently
    \item \textbf{Team Collaboration:} RL researchers can edit YAML, game designers can edit ScriptableObject
\end{itemize}

\subsubsection{Key Configuration Values}
\textbf{Movement:}
\begin{itemize}
    \item Jog speed: 6 units/sec
    \item Sprint speed: 12 units/sec
    \item Roll speed: 18 units/sec (1.5× sprint)
\end{itemize}

\textbf{Stamina System:}
\begin{itemize}
    \item Max stamina: 100.0
    \item Sprint consumption: 20.0/sec
    \item Jump cost: 20.0 per jump
    \item Roll cost: 60.0 per roll
    \item Regen rate: 30.0/sec (when not sprinting/jumping/rolling)
\end{itemize}

\textbf{Rewards:}
\begin{itemize}
    \item Progress: 0.1 per unit forward
    \item Target reach: +10.0
    \item Roll base: +0.5 (always given)
    \item Roll style bonus: +1.5 (in 40\% of episodes)
\end{itemize}

\end{document}
