Problem Statement
This project addresses autonomous navigation across procedurally generated parkour environments where agents must balance multiple competing objectives: speed (reaching targets efficiently), energy management (stamina conservation), and aesthetic quality (stylistic movement). The core challenge extends beyond standard navigation: the agent should not only reach the target but do so with human-preferred behaviors like dynamic rolls and varied movement patterns.
The Human Feedback Challenge
The ideal approach would employ Reinforcement Learning from Human Feedback (RLHF), where humans directly label preferred trajectories. However, this methodology faces fundamental incompatibility with our training infrastructure: 28 parallel agents operating at 20Ã— time acceleration generate ~1,054 steps/second, producing approximately 30 complete episodes per minute. Real-time human evaluation at this scale is infeasible, creating a critical gap between optimal methods and practical constraints.
Contribution: Stochastic Reward Shaping
We propose episodic stochastic reward modulation as a practical approximation of preference diversity. Rather than uniform reward signals, we inject randomness at the episode level: 40% of training episodes provide enhanced rewards (+1.5 bonus) for high-cost stylistic actions (rolls), while the remaining 60% offer only base rewards (+0.5). This approach attempts to model the variance in human aesthetic preferences without requiring real-time feedback.
Key insight: By creating episodes where certain behaviors are disproportionately rewarded, we force the agent to learn those behaviors remain viable strategies, preventing complete dismissal of high-cost actions that humans would find preferable.
Empirical Validation
Across multiple training configurations (2M steps each), we observe:
Baseline without style rewards: 0.69% roll usage
Stochastic reward shaping (40% bonus episodes): significantly increased roll usage
Final performance: +89.18 average reward, 632+ units traveled (550% beyond minimum target)
