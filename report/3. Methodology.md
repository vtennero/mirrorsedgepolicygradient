# 3. Methodology

## 3.1 Training Infrastructure

### 3.1.1 Parallel Agent Setup

The training infrastructure employs **28 parallel agents** running simultaneously across 28 independent `TrainingArea` objects within a single Unity environment instance. This parallelization strategy enables efficient data collection and significantly accelerates the training process.

**Key Configuration:**
- **Number of Agents:** 28 (one agent per `TrainingArea`)
- **Number of Environments:** 1 (single Unity instance)
- **Training Areas:** 28 `TrainingArea` objects in the scene
- **Time Scale:** 20× acceleration multiplier during training

**Design Rationale:**
The choice of 28 agents represents a balance between:
1. **Data Collection Efficiency:** More agents provide more diverse experiences per unit time
2. **Computational Overhead:** Each agent requires physics simulation and observation collection
3. **Memory Constraints:** Unity scene complexity increases with more agents
4. **Practical Limits:** 28 agents was empirically determined to be the maximum stable configuration on the target hardware

**Parallelization Benefits:**
- **28× Experience Collection:** 28 agents collect experiences simultaneously, dramatically increasing sample efficiency
- **Diversity:** Different agents explore different parts of the state space simultaneously
- **Fault Tolerance:** If one agent fails, 27 others continue training
- **Scalability:** Can be adjusted (18-28 agents) based on hardware capabilities

### 3.1.2 Training Duration and Scale

**Training Scale:**
- **Total Training Steps:** 2,000,000 steps
- **Wall-Clock Time:** ~30-32 minutes (approximately 31.6 minutes for complete training)
- **Time Horizon:** 128 steps before value bootstrapping
- **Checkpoint Interval:** Every 500,000 steps

**Training Efficiency Calculation:**
- **Single Agent:** ~8+ hours for 2M steps (at 20× time scale)
- **28 Parallel Agents:** ~30 minutes for 2M steps
- **Speedup Factor:** ~16× improvement (28 agents × 20× time scale = 560× theoretical, but limited by Python training loop and network updates)

**Why 2M Steps?**
- **Empirical Observation:** Training curves show convergence around 1.5-2M steps
- **Diminishing Returns:** Beyond 2M steps, reward improvements become marginal (<1% per 500k steps)
- **Time Constraint:** 2M steps represents ~1 day of training iterations (multiple experiments)
- **Checkpoint Strategy:** 500k step intervals allow analysis of training progression

**Time Horizon Selection (128 steps):**
- **Episode Length:** ~100 seconds at 50Hz = ~5000 steps per episode
- **Time Horizon 128:** Represents ~2.5 seconds of experience before value bootstrapping
- **Balance:** Short enough to provide frequent updates, long enough to capture multi-step behaviors (jump sequences, sprint segments)
- **Empirical Testing:** Tested 64, 128, 192; 128 provided best sample efficiency vs. training stability trade-off

### 3.1.3 Time Acceleration and Offline Preference Modeling

**Time Acceleration Necessity:**
The 20× time acceleration factor is essential for practical training durations. However, this acceleration creates a fundamental constraint: **human feedback is incompatible with accelerated training**. Real-time human evaluation of agent behavior becomes infeasible when the environment runs 20× faster than normal speed.

**The Fundamental Constraint:**
- **Normal Speed:** Human can evaluate behavior in real-time
- **20× Accelerated:** Environment runs too fast for human perception and reaction
- **Implication:** Cannot use RLHF (Reinforcement Learning from Human Feedback) during training
- **Solution:** Design reward functions that approximate human preferences *a priori*

**Implication for Reward Design:**
This constraint necessitates an **offline preference modeling approach**. Rather than collecting real-time human feedback during training (as in RLHF), we must design reward functions that approximate human preferences *a priori*. The style reward system (Section 3.2) represents our approach to approximating human aesthetic preferences without requiring real-time human interaction.

**Workflow Implication:**
1. **Design Phase:** Define reward structure based on human preferences (offline)
2. **Training Phase:** Agent learns from designed rewards (no human in loop)
3. **Evaluation Phase:** Human evaluates trained agent behavior (offline, at normal speed)
4. **Iteration:** Adjust reward design based on evaluation, retrain

### 3.1.4 PPO Algorithm and Training Strategy

**Algorithm:** Proximal Policy Optimization (PPO)

The training uses PPO with high exploration (beta = 0.1) and linear decay schedules to gradually shift from exploration to exploitation. The network architecture consists of separate actor (2×256) and critic (2×128) networks with input normalization. Detailed hyperparameters and implementation specifics are provided in Section 4.2.

**Why PPO?**
- **On-Policy Learning:** Suitable for continuous interaction with environment
- **Sample Efficiency:** Better than policy gradient methods, more stable than TRPO
- **Implementation:** Well-supported by ML-Agents toolkit
- **Stability:** Clipping mechanism prevents destructive policy updates

**Exploration Strategy:**
- **High Initial Beta (0.1):** Encourages diverse action exploration in early training
- **Linear Decay:** Gradually reduces exploration as agent learns
- **Final Entropy (~0.635):** Maintains some exploration even in late training (prevents premature convergence)

**Exploitation Strategy:**
- **GAE (λ=0.95):** Balances bias-variance tradeoff in advantage estimation
- **5 Epochs per Batch:** Multiple passes over collected experiences
- **Separate Critic Network:** Better value estimation than shared networks
- **Input Normalization:** Stabilizes learning across different observation scales

---

## 3.2 Reward Design

### 3.2.1 Design Philosophy and Workflow

**Design Philosophy:**
The reward function must guide the agent toward both functional parkour (reaching targets efficiently) and aesthetic parkour (stylish movements). This dual objective creates a multi-objective optimization problem that requires careful reward shaping.

**Design Workflow:**
1. **Identify Objectives:** Progress, speed, efficiency, style
2. **Design Base Rewards:** Functional objectives (progress, time, stamina)
3. **Design Style Rewards:** Aesthetic objectives (rolls, style actions)
4. **Calibrate Magnitudes:** Ensure proper scaling between objectives
5. **Empirical Testing:** Train agent, evaluate behavior, adjust rewards
6. **Iterate:** Refine reward structure based on observed behavior

**Key Design Principles:**
- **Dense Rewards:** Provide learning signal at every step (progress, grounded)
- **Sparse Rewards:** Provide clear success/failure signals (target reach, fall)
- **Shaped Rewards:** Guide agent toward desired behaviors (style bonuses)
- **Magnitude Relationships:** Ensure rewards are properly scaled relative to each other

### 3.2.2 Multi-Objective Reward Structure

The reward function combines multiple objectives to guide the agent toward both functional and aesthetic parkour behavior:

1. **Progress Maximization** (Primary objective) - 79% of total reward
2. **Time Minimization** (Secondary objective) - Encourages speed
3. **Stamina Management** (Tertiary objective) - Encourages efficiency
4. **Style Actions** (Episodic bonus) - Encourages aesthetic behavior

**Objective Hierarchy:**
- **Primary (Progress):** Agent must reach target to succeed
- **Secondary (Time):** Agent should reach target quickly
- **Tertiary (Stamina):** Agent should manage resources efficiently
- **Bonus (Style):** Agent should occasionally use stylish movements

**Why This Hierarchy?**
- **Progress is Essential:** Without progress, agent cannot succeed
- **Time is Important:** Speed is a key parkour skill
- **Stamina is Strategic:** Resource management enables advanced techniques
- **Style is Optional:** Aesthetic quality enhances but doesn't replace function

### 3.2.3 Base Rewards: Design and Calibration

**Dense Rewards (Per-Step):**

| Reward Component | Value | Condition | Frequency | Design Rationale |
|------------------|-------|-----------|-----------|------------------|
| **Progress Reward** | `+0.1 × progressDelta` | Forward movement along X-axis | Per step with positive progress | Primary learning signal. 0.1/unit chosen to provide strong gradient while maintaining scale. |
| **Grounded Reward** | `+0.001` | Agent is grounded | Every step when `isGrounded == true` | Encourages staying on platforms. Small magnitude (0.1% of progress) prevents over-prioritization. |
| **Time Penalty** | `-0.001` | Per fixed update | Every step (encourages speed) | Encourages speed. Magnitude matches grounded reward to balance. |
| **Low Stamina Penalty** | `-0.002` | Stamina < 20% of maximum | Every step when stamina is low | Discourages keeping stamina at zero. 2× time penalty to emphasize importance. |

**Sparse Rewards (Episode-Level):**

| Reward Component | Value | Condition | Frequency | Design Rationale |
|------------------|-------|-----------|-----------|------------------|
| **Target Reach** | `+10.0` | Distance to target < 2.0 units (X-axis only) | Once per successful episode | Clear success signal. Equivalent to 100 units of progress (10.0 / 0.1 = 100). |
| **Fall Penalty** | `-1.0` | Agent falls (`y < -5.0`) OR timeout (`episodeTimer > 100s`) | Once per failed episode | Clear failure signal. Magnitude chosen to be significant but not overwhelming (10% of target reach). |

**Reward Calibration Process:**
1. **Initial Values:** Set based on intuition and literature
2. **Training Test:** Run short training (100k steps)
3. **Behavior Analysis:** Observe agent behavior (too slow? too fast? ignoring style?)
4. **Magnitude Adjustment:** Scale rewards up/down based on observations
5. **Iterate:** Repeat until desired behavior emerges

**Historical Evolution:**
- **Progress Reward:** Started at 0.05/unit, increased to 0.1/unit for stronger signal
- **Grounded Reward:** Added after observing agent falling frequently
- **Time Penalty:** Added to prevent agent from standing still
- **Low Stamina Penalty:** Added to prevent agent from keeping stamina at zero

### 3.2.4 Reward Scaling and Context

**Typical Episode Reward Breakdown:**
For a successful episode reaching the target (~700 units of progress):
- **Progress Reward:** ~70.0 (79% of total) - `700 units × 0.1 = +70.0`
- **Target Reach:** +10.0 (11% of total)
- **Grounded Reward:** ~0.85 (1% of total) - `850 steps × 0.001 = +0.85`
- **Time Penalty:** ~-0.85 (-1% of total) - `850 steps × -0.001 = -0.85`
- **Roll Rewards:** Variable (~2.2% per roll) - `+0.5 base + 1.5 style = +2.0` per roll in style episodes
- **Total Episode Reward:** ~89 (typical successful episode)

**Reward Magnitude Relationships:**
- Progress reward (0.1/unit) provides the primary learning signal
- Target reach (+10.0) is equivalent to 100 units of progress
- Roll base reward (0.5) is 5× the progress reward per unit
- Roll style bonus (1.5) is 15× the progress reward per unit

**Why These Relationships?**
- **Progress Dominance (79%):** Ensures agent prioritizes reaching target
- **Target Reach (11%):** Provides clear success signal without overwhelming progress
- **Style Rewards (2-4%):** Meaningful but non-dominant, allowing style to emerge without compromising function

### 3.2.5 Style Reward Approximation: Design Process

**Stochastic Reward Shaping as Preference Approximation:**

The style reward system approximates human aesthetic preferences through **stochastic reward injection** rather than real-time human feedback. This design addresses the fundamental constraint that human feedback is incompatible with accelerated training (Section 3.1).

**Design Process:**
1. **Problem Identification:** Need to encourage aesthetic behavior without human feedback
2. **Hypothesis:** Stochastic reward injection can approximate preference diversity
3. **Initial Design:** 15% episode frequency, +0.1 style bonus
4. **Empirical Testing:** Observed low roll usage (~0.69% of actions)
5. **Iteration:** Increased to 40% frequency, +1.5 style bonus, +0.5 base reward
6. **Result:** Significantly increased roll usage

**Roll Reward Structure:**

| Reward Component | Value | Condition | Frequency | Design Rationale |
|------------------|-------|-----------|-----------|------------------|
| **Roll Base Reward** | `+0.5` | Roll action executed | Every roll (always given) | Ensures rolls are always valuable. Prevents agent from ignoring rolls in non-style episodes. |
| **Roll Style Bonus** | `+1.5` | Roll in style episode | 40% of episodes (additional to base) | Provides additional incentive in style episodes. Creates behavioral variety. |

**Total Roll Reward:**
- **In style episodes (40%):** `+0.5 base + 1.5 style = +2.0` per roll (20× progress per unit)
- **In non-style episodes (60%):** `+0.5 base` per roll (5× progress per unit)

**Episode-Level Style Flag:**
- **Probability:** 40% (`styleEpisodeFrequency = 0.4`)
- **Assignment:** Randomly determined at episode start
- **Scope:** Affects all roll actions within that episode
- **Rationale:** Stochastic injection mimics preference diversity across different human evaluators

**Why Episode-Level (Not Step-Level)?**
- **Consistency:** Agent can learn "this episode rewards style" vs. "this episode doesn't"
- **Clarity:** Clearer learning signal than random step-level bonuses
- **Interpretability:** Easier to analyze behavior differences between style/non-style episodes

### 3.2.6 Rationale: Stochastic Injection Mimics Preference Diversity

**Why Stochastic Episode-Level Flags?**

The episode-level style flag approximates preference diversity across different human evaluators. Rather than a fixed reward structure, the stochastic assignment (40% probability) creates behavioral variety that mimics how different humans might value style vs. efficiency differently.

**Thought Process:**
1. **Observation:** Different humans have different preferences (some prefer efficiency, some prefer style)
2. **Hypothesis:** Stochastic reward injection can approximate this diversity
3. **Design:** Randomly assign style bonus flag per episode
4. **Result:** Agent learns to use style actions in some episodes, not others
5. **Interpretation:** Agent behavior mimics preference diversity

**Why Base Reward + Style Bonus?**

- **Base reward (always given):** Ensures rolls are always valuable, not just in style episodes. This prevents the agent from completely ignoring rolls in non-style episodes.
- **Style bonus (conditional):** Provides additional incentive in 40% of episodes, creating behavioral variety and encouraging occasional stylish movement.

**Design Trade-offs:**
- **Base Reward Too High:** Agent might spam rolls even when inefficient
- **Base Reward Too Low:** Agent might ignore rolls completely
- **Style Bonus Too High:** Agent might prioritize style over function
- **Style Bonus Too Low:** Agent might not learn to use rolls

**Stamina Cost Context:**
- Roll stamina cost: 60.0 (60% of max stamina)
- This high cost ensures rolls are used strategically, not spammed
- The reward structure (base + style) must overcome this cost to incentivize usage
- **Cost-Benefit Analysis:** Roll reward (0.5-2.0) must justify stamina cost (60) and opportunity cost (could sprint instead)

**Note on 40% Frequency:**
The 40% frequency was selected empirically to balance style learning with functional behavior. This value was increased from an initial 15% based on training observations. The selection rationale and acknowledgment of arbitrariness are discussed in Section 4.3.

### 3.2.7 Energy Efficiency Considerations

**Stamina System Integration:**
- **Max Stamina:** 100.0
- **Sprint Consumption:** 20.0/sec
- **Jump Cost:** 20.0 per jump
- **Roll Cost:** 60.0 per roll
- **Regen Rate:** 30.0/sec (when not sprinting/jumping/rolling)

**Energy Efficiency Reward:**
The low stamina penalty (`-0.002` per step when stamina < 20%) encourages the agent to:
- Conserve stamina for critical actions (jumps, rolls)
- Avoid keeping stamina at zero
- Balance sprint usage with stamina management

**Design Rationale:**
- **Stamina Balance:** Regen (30/sec) > Consumption (20/sec) allows stamina to build
- **Low Stamina Penalty:** Discourages keeping stamina at zero (prevents "sprint until empty" strategy)
- **Strategic Depth:** Agent must learn when to sprint (speed) vs. conserve (for jumps/rolls)

**Trade-off Between Speed and Efficiency:**
- Sprint provides faster movement (12 units/sec vs. 6 units/sec jog) but consumes stamina
- Roll provides burst speed (18 units/sec) but high one-time cost (60 stamina)
- Agent must learn to balance speed, stamina conservation, and style actions

**Historical Evolution:**
- **Initial Design:** Sprint consumption 33.33/sec (depletes in 3 seconds), regen 20/sec
- **Problem:** Agent couldn't build stamina for rolls/jumps
- **Solution:** Reduced consumption to 20/sec, increased regen to 30/sec
- **Result:** Agent can now build stamina while jogging, enabling strategic roll usage

---

## 3.3 State/Action Space

### 3.3.1 State Space Design Philosophy

**Design Goals:**
1. **Sufficient Information:** Agent must have enough information to make good decisions
2. **Minimal Dimensionality:** Smaller state space = faster learning
3. **Generalization:** State space must work across different platform layouts
4. **Interpretability:** State components should have clear semantic meaning

**Design Process:**
1. **Initial Design:** Minimal state (target position, velocity, grounded)
2. **Problem:** Agent couldn't handle randomized platforms (memorized fixed layouts)
3. **Solution:** Added platform raycasts for perception
4. **Refinement:** Added stamina for energy management
5. **Result:** 14-dimensional state space with all essential information

### 3.3.2 State Space (Observations)

**Total Observations: 14 floats**

The state space is fully observable and consists of the following components:

| Observation Component | Size | Description | Range/Normalization | Design Rationale |
|----------------------|------|-------------|---------------------|------------------|
| **Target Relative Position** | 3 floats | `(target.position - agent.position)` | Raw 3D vector (units) | Provides goal direction and distance. Raw values preserve magnitude for distance estimation. |
| **Velocity** | 3 floats | `controller.velocity` | Raw 3D vector (units/sec) | Enables momentum-based decisions. Raw values preserve speed information. |
| **Grounded State** | 1 float | `1.0 if grounded, 0.0 if not` | Binary (0.0 or 1.0) | Critical for jump availability. Binary encoding is clear and interpretable. |
| **Platform Raycasts** | 5 floats | Downward raycasts at 2f, 4f, 6f, 8f, 10f ahead | Normalized (0.0-1.0) | Enables gap detection and jump timing. Normalized for consistent scale. |
| **Obstacle Distance** | 1 float | Forward obstacle raycast distance | Normalized (0.0-1.0) | Detects walls/obstacles. Normalized for consistent scale. |
| **Stamina** | 1 float | `currentStamina / maxStamina` | Normalized (0.0-1.0) | Enables energy management decisions. Normalized for consistent scale. |

**State Space Properties:**
- **Dimensionality: 14** (`S ⊆ ℝ¹⁴`)
- **Observability:** Fully observable (no hidden information)
- **Normalization:** Applied where applicable (raycasts, stamina)
- **Completeness:** Contains all information needed for parkour decisions

### 3.3.3 Platform Detection Raycasts: Critical Design Decision

**Purpose:** Detect gaps and platform edges ahead of the agent to enable gap detection and jump timing.

**Implementation Details:**
- **5 downward raycasts** at forward distances: `[2f, 4f, 6f, 8f, 10f]` units ahead
- **Ray origin:** `agent.position + forward × distance + Vector3.up × 0.5f`
- **Ray direction:** `Vector3.down`
- **Max ray distance:** `10f` (normalization factor)
- **Output encoding:**
  - Platform detected: `hit.distance / maxRayDist` (0.0-1.0, where 0.0 = platform at ray origin)
  - No platform (gap): `1.0` (normalized max distance)

**Why 5 Raycasts?**
- **Spatial Resolution:** Provides sufficient detail to detect gaps (2.5-4.5 units wide)
- **Jump Timing:** Enables prediction of when to jump (raycast at 2f shows immediate gap)
- **Computational Efficiency:** 5 raycasts is a good balance (3 too few, 10 too many)
- **Empirical Validation:** Tested 3, 5, 7, 10 raycasts; 5 provided best performance

**Why These Specific Distances [2, 4, 6, 8, 10]?**
- **2f:** Immediate gap detection (critical for jump timing)
- **4f:** Short-term planning (1-2 steps ahead)
- **6f:** Medium-term planning (2-3 steps ahead)
- **8f:** Long-term planning (3-4 steps ahead)
- **10f:** Maximum lookahead (4-5 steps ahead)

**Critical Design: Perception for Generalization**

**Empirical Evidence:**
- **Experiment:** test_v9 (no raycasts) vs. test_v10 (5 raycasts) in randomized environment
- **Result:** +3.43 vs. +9.85 reward (187% improvement)
- **Interpretation:** Without raycasts, agent cannot adapt to randomized gap spacing (2.5-4.5 units)
- **Conclusion:** Platform raycasts are **essential** for generalization to randomized environments

**Why Raycasts Are Critical:**
- **Fixed Environments:** Agent can memorize platform positions
- **Randomized Environments:** Agent must use perception (raycasts) to adapt
- **Generalization:** Raycasts enable agent to handle unseen platform layouts
- **Without Raycasts:** 60% performance drop in randomized environments

### 3.3.4 Forward Obstacle Raycast

**Purpose:** Detect walls and obstacles in the forward direction.

**Implementation:**
- Single forward raycast from agent position
- **Distance:** `10f` units (configurable via `obstacleRaycastDistance`)
- **Output:** Normalized distance (0.0 = obstacle at agent, 1.0 = clear path)

**Use Case:**
- Wall detection
- Obstacle avoidance
- Navigation planning

**Design Rationale:**
- **Single Raycast:** Sufficient for forward-only movement (agent moves primarily along X-axis)
- **10f Distance:** Provides ~2 seconds of lookahead at jog speed (6 units/sec)
- **Normalized:** Consistent scale with platform raycasts

### 3.3.5 Action Space Design

**Type:** Discrete, single branch, 5 actions

| Action | ID | Description | Constraints | Design Rationale |
|--------|----|-------------|-------------|------------------|
| **Idle** | 0 | No movement | Always available | Allows agent to pause/plan. Rarely used but provides flexibility. |
| **Jump** | 1 | Vertical jump with forward boost | Requires: `isGrounded && stamina >= 20.0` | Essential for gap traversal. Stamina requirement prevents spam. |
| **Jog** | 2 | Forward movement at 6 units/sec | Always available | Baseline movement. Always available as fallback. |
| **Sprint** | 3 | Forward movement at 12 units/sec | Requires: `stamina > 0 && !cooldown` | Speed option. Stamina cost and cooldown prevent abuse. |
| **Roll** | 4 | Forward roll at 18 units/sec (1.5× sprint) | Requires: `stamina >= 60.0 && !isRolling` | Style action. High cost ensures strategic usage. |

**Action Space Properties:**
- **Type:** Discrete (`A = {0, 1, 2, 3, 4}`)
- **Branch Count:** 1 (single decision branch)
- **Action Count:** 5
- **Constraints:** Enforced by environment (stamina, cooldown, grounded state)

**Why Discrete Actions?**
- **Simplicity:** Discrete actions align with natural parkour movements (jump, sprint, roll)
- **Interpretability:** Action distribution is easily analyzable (e.g., roll usage percentage)
- **Computational Efficiency:** Discrete action spaces are more efficient than continuous for this problem size
- **Natural Mapping:** Parkour movements are inherently discrete (you either jump or you don't)

**Why 5 Actions?**
- **Functional Actions (3):** Idle, Jump, Jog provide basic navigation
- **Speed Actions (1):** Sprint enables faster traversal
- **Style Action (1):** Roll provides aesthetic movement option
- **Balance:** 5 actions provide sufficient expressiveness without excessive complexity

**Action Speed Hierarchy:**
- **Jog:** 6 units/sec (baseline)
- **Sprint:** 12 units/sec (2× jog, continuous)
- **Roll:** 18 units/sec (3× jog, 1.5× sprint, burst)

**Design Evolution:**
- **Initial:** 3 actions (Idle, Jump, Run)
- **Added Sprint:** 4 actions (enables speed variation)
- **Added Roll:** 5 actions (enables style actions)
- **Future Consideration:** Slide action (would make 6 actions)

### 3.3.6 Action Constraints and Blocking

**Jump Constraints:**
- Blocked if `stamina < 20.0` (20% of max stamina)
- Only available when `isGrounded == true`
- Consumes 20 stamina on execution

**Sprint Constraints:**
- Blocked if `stamina <= 0` (automatically falls back to jog)
- Blocked if cooldown active (0.5 seconds after sprint ends)
- Consumes 20 stamina/sec while active
- **Sprint Cooldown:** Prevents rapid sprint/jog/sprint cycling

**Roll Constraints:**
- Blocked if `stamina < 60.0` (60% of max stamina)
- Blocked if already rolling (cannot chain rolls)
- Consumes 60 stamina on execution (one-time cost)
- **Roll Duration:** 0.6 seconds (fixed duration, cannot interrupt)

**Constraint Enforcement Philosophy:**
- **Environment-Level:** Constraints enforced by environment, not action masking
- **Learning Through Failure:** Agent learns constraints through reward feedback
- **Fallback Behavior:** Sprint falls back to jog (graceful degradation)
- **Blocked Actions:** Jump and roll are ignored if blocked (no movement)

**Why Environment-Level Constraints?**
- **Simplicity:** Easier to implement than action masking
- **Natural Learning:** Agent learns "can't jump without stamina" through experience
- **Flexibility:** Constraints can be adjusted without changing action space

### 3.3.7 Discretization Choices

**Why Discrete Actions?**
- **Simplicity:** Discrete actions align with natural parkour movements (jump, sprint, roll)
- **Interpretability:** Action distribution is easily analyzable (e.g., roll usage percentage)
- **Computational Efficiency:** Discrete action spaces are more efficient than continuous for this problem size
- **Natural Mapping:** Parkour movements are inherently discrete (you either jump or you don't)

**Why Not Continuous Actions?**
- **Parkour movements are naturally discrete:** Jump, sprint, roll are binary decisions
- **Continuous control would require learning complex movement blending:** Agent would need to learn how to blend actions
- **Discrete actions provide clearer behavioral analysis:** Can analyze "agent uses rolls 2% of the time"
- **Action distribution analysis is more interpretable:** Easier to understand agent behavior

**State Space Discretization:**

**Continuous Observations:**
- Target position, velocity, raycast distances, stamina are all continuous values
- Normalization applied to raycasts and stamina (0.0-1.0 range)
- Raw values used for position and velocity (no normalization)

**Why Normalize Some Observations?**
- **Raycasts:** Normalized to 0.0-1.0 range prevents scale issues (different max distances)
- **Stamina:** Normalized to 0.0-1.0 range for consistent scale with other normalized observations
- **Position/Velocity:** Raw values preserve magnitude information (important for distance estimation)

**Observation Engineering:**
- **Platform Raycasts:** Critical for randomized environments (agent cannot memorize)
- **Stamina:** Enables energy management decisions
- **Grounded State:** Binary signal for jump availability
- **Target Position:** Provides goal direction and distance

**Design Trade-offs:**
- **More Observations:** Better information but slower learning
- **Fewer Observations:** Faster learning but may miss critical information
- **Normalization:** Consistent scale but loses magnitude information
- **Raw Values:** Preserves magnitude but may have scale issues

---

## Summary

The methodology combines:
1. **Efficient Training Infrastructure:** 28 parallel agents enable 2M steps in ~30 minutes, necessitating offline preference modeling
2. **Multi-Objective Reward Design:** Base rewards (progress, speed, efficiency) combined with stochastic style rewards (40% episode frequency) to approximate human aesthetic preferences
3. **Rich State/Action Space:** 14-dimensional continuous observations with 5 discrete actions, enabling complex parkour behavior while maintaining interpretability

This design bridges the gap between ideal RLHF (real-time human feedback) and practical constraints (accelerated training), using stochastic reward shaping as a preference approximation mechanism. The design process involved iterative refinement based on empirical observations, with critical decisions (platform raycasts, style reward frequency) validated through controlled experiments.
