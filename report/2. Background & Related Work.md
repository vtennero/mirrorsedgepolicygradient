Problem: Multi-objective parkour with aesthetic quality
Challenge: Human feedback incompatible with accelerated training
Contribution: Stochastic reward shaping as preference approximation

real research question
How do you integrate human preferences into RL when human reaction time is orders of magnitude slower than agent training time?


Real-time human feedback is fundamentally incompatible with accelerated simulation training. At 28 agents × accelerated time, a human would need to provide feedback for 28×speedup factor actions per second. We address this by modeling preference diversity through stochastic reward injection, where style bonuses appear in 40% of episodes. This approximates the distribution of human aesthetic preferences without requiring infeasible real-time annotation.

We initially attempted real-time human feedback but found it incompatible with accelerated multi-agent training (28 agents, 2M steps/30min). As a first-order approximation, we model preference diversity through stochastic style reward injection. This pragmatic approach enabled rapid iteration while establishing the behavioral foundation for future offline RLHF integration.

We prioritized rapid prototyping of environment and agent capabilities over reward model complexity. This enabled validation of core mechanics within tight time constraints. RLHF integration represents the natural next phase now that behavioral baselines are established.

