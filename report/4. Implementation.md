# 4. Implementation

## 4.1 Unity ML-Agents Setup

### 4.1.1 Environment Configuration

The training environment is built using **Unity 2022.3 LTS** with the **ML-Agents Toolkit (version 1.1.0)**. The implementation follows the standard ML-Agents architecture with custom extensions for parkour-specific behaviors.

**Core Components:**
- **Agent Script:** `ParkourAgent.cs` - Inherits from `Unity.MLAgents.Agent`
- **Training Areas:** 28 `TrainingArea` objects in the scene (one per parallel agent)
- **Character Controller:** Unity's built-in `CharacterController` component for physics-based movement
- **Configuration System:** `CharacterConfig` ScriptableObject for centralized parameter management

**Unity Project Structure:**
```
src/
├── Assets/
│   ├── Scripts/
│   │   ├── ParkourAgent.cs          # Main RL agent
│   │   ├── TrainingArea.cs          # Environment generator
│   │   ├── CharacterConfig.cs       # Configuration ScriptableObject
│   │   └── [other scripts]
│   ├── Scenes/
│   │   └── TrainingScene.unity       # Training scene with 28 agents
│   └── Prefabs/
│       └── TrainingArea.prefab       # Reusable training area prefab
├── parkour_config.yaml               # ML-Agents training config
└── train_with_progress.py            # Training wrapper script
```

**ML-Agents Integration:**
- **Package Version:** `com.unity.ml-agents` 3.0.0+ (Unity Package Manager)
- **Python Package:** `mlagents` 1.1.0 (via conda/pip)
- **Communication:** Unity ↔ Python via gRPC on port 5004 (default)
- **Behavior Name:** `ParkourRunner` (must match in config and Unity)

### 4.1.2 Agent Implementation Architecture

**`ParkourAgent.cs` Key Features:**
- **Observation Collection:** Implements `CollectObservations()` to gather 14-dimensional state vector
- **Action Execution:** Implements `OnActionReceived()` to process discrete actions and apply constraints
- **Reward Calculation:** Computes multi-objective rewards in `FixedUpdate()` and `OnActionReceived()`
- **Episode Management:** Handles episode reset, termination conditions, and style bonus flag assignment

**ML-Agents Lifecycle Integration:**

```csharp
// Unity ML-Agents Agent lifecycle
public override void Initialize()           // Called once at start
public override void OnEpisodeBegin()      // Called at episode start
public override void CollectObservations()  // Called every decision step
public override void OnActionReceived()    // Called every decision step
void FixedUpdate()                         // Unity physics update (50Hz)
```

**Behavior Parameters Configuration:**
- **Behavior Name:** `ParkourRunner` (must match `parkour_config.yaml`)
- **Vector Observation Space:** 14 floats (configured in Unity Inspector)
- **Discrete Action Space:** 1 branch, 5 actions (Idle, Jump, Jog, Sprint, Roll)
- **Decision Requester:** Requests decisions every fixed update (50Hz by default)
- **Behavior Type:** `Default` (training) or `InferenceOnly` (demo mode)

**Component Dependencies:**
- **CharacterController:** Unity built-in component for physics-based movement
- **DecisionRequester:** ML-Agents component that requests decisions from policy
- **BehaviorParameters:** ML-Agents component that defines observation/action spaces
- **CharacterConfigManager:** Singleton providing access to `CharacterConfig` ScriptableObject

### 4.1.3 Training Workflow: Detailed Process

**Training Command:**
```bash
cd src
conda activate mlagents
python train_with_progress.py parkour_config.yaml --run-id=training_<timestamp> --force
```

**Step-by-Step Training Process:**

**Phase 1: Initialization (0-5 seconds)**
1. **Python Script Execution:**
   - `train_with_progress.py` reads `parkour_config.yaml`
   - Parses max_steps (2,000,000) and behavior name (`ParkourRunner`)
   - Auto-generates run-id: `training_YYYYMMDD_HHMMSS`
   - Launches `mlagents-learn` subprocess with config file

2. **ML-Agents Trainer Startup:**
   - Python trainer initializes PyTorch model (actor + critic networks)
   - Opens gRPC server on port 5004
   - Waits for Unity connection

3. **Unity Editor Connection:**
   - User opens `TrainingScene.unity` in Unity Editor
   - Scene contains 28 `TrainingArea` objects, each with a `ParkourAgent`
   - User presses **Play** button in Unity Editor
   - Unity ML-Agents connects to Python trainer on port 5004

**Phase 2: Training Loop (30 minutes)**
4. **Experience Collection:**
   - Each of 28 agents collects experiences simultaneously
   - At 50Hz (20ms per step), each agent:
     - Collects observations (14 floats)
     - Receives action from policy network
     - Executes action (with constraints)
     - Calculates rewards
     - Stores experience tuple: `(state, action, reward, next_state)`

5. **Batch Processing:**
   - When buffer reaches `time_horizon` (128 steps) × 28 agents = 3,584 experiences:
     - Trainer samples `batch_size` (1024) experiences
     - Computes advantages using GAE (λ=0.95, γ=0.99)
     - Trains policy network for `num_epoch` (5) epochs
     - Updates value network (critic)
     - Clears buffer, continues collection

6. **Progress Tracking:**
   - `train_with_progress.py` intercepts ML-Agents output
   - Parses step count from log lines: `[INFO] ParkourRunner. Step: 680000.`
   - Calculates percentage: `(current_steps / max_steps) × 100`
   - Displays: `[34.0%] Time Elapsed: 735.333 s. Mean Reward: 9.899.`

7. **Checkpointing:**
   - Every 500,000 steps: Model saved to `src/results/training_*/ParkourRunner.onnx`
   - Summary logs saved to `src/results/training_*/run_logs/`
   - TensorBoard logs updated for visualization

**Phase 3: Completion (2M steps)**
8. **Training Completion:**
   - Final model saved at 2,000,000 steps
   - Training statistics written to `timers.json` and `training_status.json`
   - Python script exits
   - Unity Editor can be stopped

**Monitoring and Debugging:**
- **TensorBoard:** `tensorboard --logdir src/results/` → `http://localhost:6006`
- **Progress Tracking:** Real-time percentage and ETA in terminal
- **Unity Console:** Debug logs from `ParkourAgent.cs` (action distribution, rewards)
- **Checkpoint Analysis:** Review `training_status.json` for reward progression

### 4.1.4 Parallel Agent Architecture

**TrainingArea Prefab Structure:**
Each `TrainingArea` GameObject contains:
- **Agent GameObject:** Child object with `ParkourAgent` component
- **Platforms:** Procedurally generated platforms (20 per episode)
- **Target:** Finish line position (dynamically calculated)
- **Spawn Point:** Agent starting position

**Spatial Arrangement:**
- 28 `TrainingArea` objects arranged in scene (spaced to avoid overlap)
- Each area is independent (separate platforms, separate agent)
- All agents share the same policy network (centralized learning)
- Experiences from all agents are aggregated in the training buffer

**Synchronization:**
- **Decision Steps:** All agents request decisions simultaneously (synchronized by ML-Agents)
- **Physics Steps:** All agents update physics in `FixedUpdate()` (Unity's fixed timestep)
- **Episode Resets:** Agents reset independently when they reach target/fall/timeout
- **Style Bonus Flags:** Each agent independently assigns style flag at episode start

---

## 4.2 Training Hyperparameters

### 4.2.1 PPO Configuration

The training uses Proximal Policy Optimization (PPO) with the following hyperparameters defined in `parkour_config.yaml`:

**Hyperparameters:**
- **Learning Rate:** `3.0e-4` (linear decay schedule)
- **Batch Size:** `1024` experiences per training batch
- **Buffer Size:** `10240` (10× batch size for experience replay)
- **Beta (Entropy):** `0.1` (linear decay) - High exploration coefficient
- **Epsilon (Clipping):** `0.2` (linear decay) - PPO clipping parameter
- **Lambda (GAE):** `0.95` - Generalized Advantage Estimation lambda
- **Gamma (Discount):** `0.99` - Discount factor for future rewards
- **Num Epochs:** `5` - Training epochs per batch
- **Time Horizon:** `128` steps before value bootstrapping

**Network Architecture:**
- **Actor Network:** 2 hidden layers × 256 units, input normalization enabled
- **Critic Network:** 2 hidden layers × 128 units, separate from actor (not shared)
- **Activation:** ReLU (default ML-Agents)
- **Initialization:** Xavier/Glorot uniform (ML-Agents default)

**Training Schedule:**
- **Max Steps:** 2,000,000
- **Summary Frequency:** Every 20,000 steps (TensorBoard logging)
- **Checkpoint Interval:** Every 500,000 steps (model saving)

### 4.2.2 Hyperparameter Selection Rationale

**High Beta (0.1):** Increased from default 0.015 to encourage exploration in the complex parkour environment. The linear decay schedule allows gradual shift from exploration to exploitation.

**Selection Process:**
- **Initial Value:** 0.015 (ML-Agents default)
- **Problem:** Agent converged too quickly, missed optimal strategies
- **Experimentation:** Tested 0.05, 0.1, 0.2
- **Result:** 0.1 provided best balance (high exploration, still learns effectively)
- **Decay:** Linear from 0.1 → ~0.00074 over 2M steps

**5 Epochs:** Balanced choice between sample efficiency (3 epochs) and training stability (12 epochs). Provides multiple passes over each batch without excessive computation.

**Selection Process:**
- **Tested Values:** 3, 5, 8, 12 epochs
- **3 Epochs:** Too few, unstable learning
- **12 Epochs:** Too many, slow training, overfitting risk
- **5 Epochs:** Sweet spot (good sample efficiency, stable learning)

**Time Horizon 128:** Balanced between shorter horizons (64) that may miss long-term dependencies and longer horizons (192) that slow training. Appropriate for 100-second episodes.

**Selection Process:**
- **Episode Length:** ~100 seconds at 50Hz = ~5000 steps
- **Tested Values:** 64, 128, 192, 256 steps
- **64 Steps:** Too short, misses multi-step behaviors (jump sequences)
- **192 Steps:** Too long, slows training, diminishing returns
- **128 Steps:** Represents ~2.5 seconds of experience (good for parkour)

**Linear Decay Schedules:** All hyperparameters (learning rate, beta, epsilon) use linear decay to gradually shift from exploration to exploitation over 2M steps.

**Decay Formulas:**
- **Learning Rate:** `lr(t) = 3.0e-4 × (1 - t / 2,000,000)`
- **Beta:** `beta(t) = 0.1 × (1 - t / 2,000,000)`
- **Epsilon:** `epsilon(t) = 0.2 × (1 - t / 2,000,000)`

### 4.2.3 Network Architecture Details

**Actor Network (Policy):**
```
Input: 14 floats (observations)
  ↓
Normalization Layer (per-feature normalization)
  ↓
Hidden Layer 1: 256 units, ReLU
  ↓
Hidden Layer 2: 256 units, ReLU
  ↓
Output: 5 logits (action probabilities)
  ↓
Softmax → Action Distribution
```

**Critic Network (Value):**
```
Input: 14 floats (observations)
  ↓
Normalization Layer (per-feature normalization)
  ↓
Hidden Layer 1: 128 units, ReLU
  ↓
Hidden Layer 2: 128 units, ReLU
  ↓
Output: 1 scalar (state value estimate)
```

**Why Separate Networks?**
- **Better Value Estimation:** Critic can focus solely on value estimation
- **Stability:** Separate networks prevent value estimation from interfering with policy
- **Flexibility:** Can adjust critic architecture independently (128 vs 256 units)

**Why Different Sizes (256 vs 128)?**
- **Actor Needs More Capacity:** Policy must learn complex action selection
- **Critic Needs Less Capacity:** Value estimation is simpler (single scalar output)
- **Computational Efficiency:** Smaller critic reduces training time

---

## 4.3 Style Episode Frequency: Why 40%?

### 4.3.1 Empirical Evolution

The style episode frequency was **increased from 15% to 40%** during development based on empirical observations:

**Initial Design (15%):**
- Original implementation used `styleEpisodeFrequency = 0.15`
- Rationale: Provide occasional style incentives without overwhelming functional objectives
- Result: Roll usage remained low (~0.69% of actions, 28.1 rolls/episode)
- **Training Run:** `training_20251207_171550` (previous run before frequency increase)

**Current Design (40%):**
- Increased to `styleEpisodeFrequency = 0.4` (40% of episodes)
- Rationale: Provide more opportunities for style actions to be learned and expressed
- Result: Significantly increased roll usage (exact percentage from training logs)
- **Training Run:** `training_20251207_210205` (current run with 40% frequency)

**Empirical Evidence:**
- **15% Frequency:** Roll usage ~0.69% of actions, agent rarely used rolls
- **40% Frequency:** Roll usage significantly increased (user confirmed "more rolls")
- **Reward Improvement:** +31% improvement (+67.90 → +89.18) with 40% frequency

### 4.3.2 Selection Criteria

The 40% frequency was chosen to balance three competing objectives:

1. **Sufficient Style Incentive:** High enough frequency to ensure the agent learns roll usage patterns
2. **Functional Behavior Preservation:** Low enough that 60% of episodes focus purely on functional objectives (progress, speed, efficiency)
3. **Behavioral Variety:** Creates diversity in agent behavior across episodes

**Decision Matrix:**

| Frequency | Style Learning | Functional Behavior | Behavioral Variety | Overall |
|-----------|---------------|-------------------|-------------------|---------|
| 10-20% | Too low | Excellent | Low | Poor |
| 30% | Moderate | Good | Moderate | Good |
| **40%** | **Good** | **Good** | **High** | **Best** |
| 50-60% | Excellent | Moderate | High | Good (risks style over function) |

### 4.3.3 Acknowledgment of Arbitrariness

**We acknowledge that the 40% value is somewhat arbitrary** and was selected through empirical tuning rather than theoretical optimization. The choice represents a practical balance point that:
- Provides sufficient style signal for learning
- Maintains functional behavior in majority of episodes
- Approximates preference diversity (different human evaluators might prefer different style/efficiency trade-offs)

**Alternative Frequencies Considered:**
- **10-20%:** Too infrequent, agent rarely learns style actions (observed in 15% experiment)
- **50-60%:** Too frequent, risks prioritizing style over function (not tested, but theoretical concern)
- **40%:** Empirical sweet spot observed in training experiments

**Theoretical Justification (Post-Hoc):**
While 40% was chosen empirically, we can justify it post-hoc:
- **Majority Functional (60%):** Ensures agent prioritizes reaching target
- **Substantial Style (40%):** Provides enough style signal for learning
- **Preference Diversity:** Mimics scenario where 40% of human evaluators prefer style, 60% prefer efficiency

### 4.3.4 Implementation Details

**Code Location:** `src/Assets/Scripts/CharacterConfig.cs`
```csharp
[Tooltip("Probability that an episode will have style bonuses enabled (0.1 = 10%, 0.2 = 20%)")]
[Range(0f, 1f)]
public float styleEpisodeFrequency = 0.4f; // Increased from 15% to 40%
```

**Assignment Logic:** `src/Assets/Scripts/ParkourAgent.cs`
```csharp
public override void OnEpisodeBegin()
{
    // ... other reset logic ...
    
    // Randomly assign style bonus flag at episode start
    styleBonusEnabled = Random.Range(0f, 1f) < config.styleEpisodeFrequency;
    
    // ... rest of reset logic ...
}
```

**Impact:** The flag is assigned once per episode and affects all roll actions within that episode. This episodic-level assignment ensures consistent reward structure throughout each episode, making it easier for the agent to learn the relationship between style episodes and roll rewards.

**Why Episode-Level (Not Step-Level)?**
- **Consistency:** Agent can learn "this episode rewards style" vs. "this episode doesn't"
- **Clarity:** Clearer learning signal than random step-level bonuses
- **Interpretability:** Easier to analyze behavior differences between style/non-style episodes
- **Implementation Simplicity:** Single random check per episode vs. per-step checks

---

## 4.4 Configuration System

### 4.4.1 Dual Configuration Architecture

The implementation uses a **dual configuration system** to separate training hyperparameters from environment/gameplay parameters:

**1. ML-Agents Config (`parkour_config.yaml`):**
- PPO hyperparameters (learning rate, batch size, etc.)
- Network architecture settings
- Training schedule (max steps, checkpoints)
- **Location:** `src/parkour_config.yaml`
- **Format:** YAML
- **Edited:** Text editor (no Unity required)

**2. Unity ScriptableObject (`CharacterConfig.cs`):**
- Movement parameters (speeds, jump force, gravity)
- Stamina system (max, consumption, regen rates)
- Reward values (progress multiplier, target reach, roll rewards)
- Environment settings (episode timeout, raycast distances)
- Style system (roll base reward, style bonus, frequency)
- **Location:** Unity Project (`Assets/Settings/CharacterConfig.asset`)
- **Format:** Unity ScriptableObject (serialized as `.asset` file)
- **Edited:** Unity Inspector (visual editor)

**Rationale:** This separation allows:
- **Training hyperparameters** to be adjusted without Unity recompilation
- **Gameplay parameters** to be tuned in Unity Editor with immediate visual feedback
- **Version control** of both configuration types independently
- **Team Collaboration:** RL researchers can edit YAML, game designers can edit ScriptableObject

### 4.4.2 Configuration Workflow

**Editing Training Hyperparameters:**
1. Open `src/parkour_config.yaml` in text editor
2. Modify hyperparameters (learning rate, batch size, etc.)
3. Save file
4. Restart training (no Unity recompilation needed)

**Editing Gameplay Parameters:**
1. Open Unity Editor
2. Select `CharacterConfig` asset in Project window
3. Modify values in Inspector (speeds, rewards, etc.)
4. Save scene (Unity auto-saves ScriptableObject)
5. Restart training (Unity recompiles C# scripts if needed)

**Version Control:**
- **YAML Config:** Tracked in git, easy to diff
- **ScriptableObject:** Tracked in git (`.asset` file), harder to diff but still versioned
- **Separate Changes:** Can modify training params without affecting gameplay params

### 4.4.3 Key Configuration Values

**Movement:**
- Jog speed: 6 units/sec
- Sprint speed: 12 units/sec
- Roll speed: 18 units/sec (1.5× sprint)

**Stamina System:**
- Max stamina: 100.0
- Sprint consumption: 20.0/sec
- Jump cost: 20.0 per jump
- Roll cost: 60.0 per roll
- Regen rate: 30.0/sec (when not sprinting/jumping/rolling)

**Rewards:**
- Progress: 0.1 per unit forward
- Target reach: +10.0
- Roll base: +0.5 (always given)
- Roll style bonus: +1.5 (in 40% of episodes)

**Environment:**
- Episode timeout: 100 seconds
- Platform count: 20 per episode
- Target offset: 5 units beyond last platform

**Historical Evolution:**
- **Sprint Consumption:** 33.33/sec → 20/sec (to allow stamina building)
- **Roll Cost:** 150 → 60 (to make rolls more accessible)
- **Regen Rate:** 20/sec → 30/sec (to enable stamina building)
- **Style Frequency:** 15% → 40% (to increase roll usage)

---

## 4.5 Technical Implementation Details

### 4.5.1 Observation Collection: Implementation Flow

**Implementation:** `ParkourAgent.CollectObservations()`

**Execution Flow:**
1. **Called by ML-Agents:** Every decision step (50Hz by default, 20ms intervals)
2. **Observation Vector:** Cleared and populated with 14 floats
3. **Order Matters:** Observations added in specific order (must match network input)

**Detailed Implementation:**

```csharp
public override void CollectObservations(VectorSensor sensor)
{
    // 1. Target Position (3 floats)
    if (target != null)
    {
        Vector3 toTarget = target.position - transform.position;
        sensor.AddObservation(toTarget); // Raw 3D vector
    }
    else
    {
        sensor.AddObservation(Vector3.zero); // Fallback if no target
    }
    
    // 2. Velocity (3 floats)
    sensor.AddObservation(controller.velocity); // Raw 3D vector
    
    // 3. Grounded State (1 float)
    sensor.AddObservation(controller.isGrounded ? 1f : 0f); // Binary
    
    // 4. Platform Raycasts (5 floats)
    float maxRayDist = 10f;
    float[] forwardDistances = { 2f, 4f, 6f, 8f, 10f };
    foreach (float forwardDist in forwardDistances)
    {
        Vector3 rayOrigin = transform.position + transform.forward * forwardDist + Vector3.up * 0.5f;
        RaycastHit hit;
        if (Physics.Raycast(rayOrigin, Vector3.down, out hit, maxRayDist))
        {
            sensor.AddObservation(hit.distance / maxRayDist); // Normalized
        }
        else
        {
            sensor.AddObservation(1f); // No platform (gap)
        }
    }
    
    // 5. Obstacle Raycast (1 float)
    float obstacleDistance = Physics.Raycast(transform.position, transform.forward, 
        out RaycastHit obstacleHit, config.obstacleRaycastDistance) 
        ? obstacleHit.distance : config.obstacleRaycastDistance;
    sensor.AddObservation(obstacleDistance / config.obstacleRaycastDistance); // Normalized
    
    // 6. Stamina (1 float)
    sensor.AddObservation(currentStamina / config.maxStamina); // Normalized
    
    // Total: 3 + 3 + 1 + 5 + 1 + 1 = 14 floats
}
```

**Performance Considerations:**
- **Raycast Cost:** 6 raycasts per step × 28 agents = 168 raycasts per step
- **Optimization:** Raycasts are relatively cheap in Unity (simple distance checks)
- **Bottleneck:** Physics simulation (28 agents) is more expensive than raycasts

### 4.5.2 Action Execution and Constraints: Detailed Flow

**Implementation:** `ParkourAgent.OnActionReceived()`

**Execution Order (Critical for Correct Behavior):**

1. **Receive Action from Policy:**
   ```csharp
   int action = actions.DiscreteActions[0]; // 0-4
   ```

2. **Store Original Action (for tracking):**
   ```csharp
   int originalAction = action; // Track before blocking
   ```

3. **Apply Constraints (in order):**
   ```csharp
   // Sprint: Check stamina and cooldown
   if (action == 3)
   {
       if (currentStamina <= 0f || IsSprintCooldownActive())
       {
           action = 2; // Fall back to jog
       }
   }
   
   // Jump: Check grounded and stamina
   if (action == 1)
   {
       if (!controller.isGrounded || currentStamina < 20f)
       {
           action = 0; // Block jump
       }
   }
   
   // Roll: Check stamina and rolling state
   if (action == 4)
   {
       if (currentStamina < 60f || isRolling)
       {
           action = 0; // Block roll
       }
   }
   ```

4. **Store Current Action:**
   ```csharp
   currentAction = action; // Used in FixedUpdate() for movement
   ```

5. **Execute One-Time Actions:**
   ```csharp
   if (currentAction == 1 && controller.isGrounded)
   {
       TriggerJump(); // Apply jump force
   }
   
   if (currentAction == 4 && controller.isGrounded && !isRolling)
   {
       TriggerRoll(); // Start roll animation/movement
   }
   ```

**Fallback Behavior:**
- **Sprint blocked → Jog:** Graceful degradation (agent still moves)
- **Jump blocked → Idle:** Action ignored (no movement, no penalty)
- **Roll blocked → Idle:** Action ignored (no movement, no penalty)

**Why Environment-Level Constraints?**
- **Simplicity:** Easier to implement than action masking
- **Natural Learning:** Agent learns constraints through experience
- **Flexibility:** Constraints can be adjusted without changing action space

### 4.5.3 Reward Calculation: Distributed Implementation

**Implementation:** Distributed across `OnActionReceived()` and `FixedUpdate()`

**Why Distributed?**
- **FixedUpdate():** Physics step (50Hz), good for per-step rewards (progress, grounded, time)
- **OnActionReceived():** Action step, good for action-specific rewards (rolls, target reach)

**Per-Step Rewards (FixedUpdate - 50Hz):**

```csharp
void FixedUpdate()
{
    // Progress Reward
    float currentX = transform.position.x;
    float progressDelta = currentX - lastProgressX;
    if (progressDelta > 0)
    {
        AddReward(progressDelta * config.progressRewardMultiplier); // +0.1 per unit
    }
    lastProgressX = currentX;
    
    // Grounded Reward
    if (controller.isGrounded)
    {
        AddReward(0.001f); // Small reward for staying on platform
    }
    
    // Time Penalty
    AddReward(config.timePenalty); // -0.001 per step
    
    // Low Stamina Penalty
    if (currentStamina / config.maxStamina < 0.2f)
    {
        AddReward(config.lowStaminaPenalty); // -0.002 per step
    }
    
    // ... physics updates (gravity, movement, stamina) ...
}
```

**Action-Specific Rewards (OnActionReceived):**

```csharp
public override void OnActionReceived(ActionBuffers actions)
{
    // ... action processing ...
    
    // Roll Rewards
    if (currentAction == 4 && isRolling)
    {
        // Base reward (always given)
        AddReward(config.rollBaseReward); // +0.5
        
        // Style bonus (conditional)
        if (styleBonusEnabled)
        {
            AddReward(config.rollStyleBonus); // +1.5
        }
    }
    
    // Target Reach (episode-level)
    if (target != null)
    {
        float distanceToTargetX = Mathf.Abs(transform.position.x - target.position.x);
        if (distanceToTargetX < config.targetReachDistance)
        {
            AddReward(config.targetReachReward); // +10.0
            EndEpisode(); // Success
        }
    }
    
    // Fall/Timeout (episode-level)
    if (transform.position.y < config.fallThreshold || episodeTimer > config.episodeTimeout)
    {
        AddReward(config.fallPenalty); // -1.0
        EndEpisode(); // Failure
    }
}
```

**Reward Timing:**
- **Progress/Grounded/Time:** Every FixedUpdate (50Hz, ~20ms intervals)
- **Roll Rewards:** Every OnActionReceived when rolling (50Hz during roll)
- **Target Reach/Fall:** Once per episode (when condition met)

### 4.5.4 Episode Management: Complete Lifecycle

**Reset Logic (OnEpisodeBegin):**

```csharp
public override void OnEpisodeBegin()
{
    // 1. Reset Position
    transform.position = startPos;
    transform.rotation = Quaternion.identity;
    
    // 2. Reset Physics
    velocity = Vector3.zero;
    controller.enabled = false; // Disable to reset
    controller.enabled = true;  // Re-enable
    
    // 3. Reset Stamina
    currentStamina = config.maxStamina;
    
    // 4. Reset Progress Tracking
    lastProgressX = startPos.x;
    maxDistanceReached = 0f;
    
    // 5. Reset Episode Timer
    episodeTimer = 0f;
    
    // 6. Reset Action State
    currentAction = 0;
    isRolling = false;
    justJumped = false;
    
    // 7. Reset Metrics
    episodeReward = 0f;
    jumpCount = 0;
    rollActionCount = 0;
    // ... other counters ...
    
    // 8. Style Bonus Flag (Random Assignment)
    styleBonusEnabled = Random.Range(0f, 1f) < config.styleEpisodeFrequency;
    
    // 9. Regenerate Platforms (if TrainingArea exists)
    if (trainingArea != null)
    {
        trainingArea.RegeneratePlatforms();
        trainingArea.UpdateTargetPosition(); // Recalculate target
    }
    
    // 10. Update Target Reference (if TrainingArea auto-assigns)
    if (trainingArea != null && target == null)
    {
        target = trainingArea.GetTargetTransform();
    }
}
```

**Termination Conditions:**

**Success Condition:**
```csharp
float distanceToTargetX = Mathf.Abs(transform.position.x - target.position.x);
if (distanceToTargetX < config.targetReachDistance) // 2.0 units
{
    AddReward(config.targetReachReward); // +10.0
    EndEpisode(); // Triggers OnEpisodeBegin() for next episode
}
```

**Failure Conditions:**
```csharp
// Fall Detection
if (transform.position.y < config.fallThreshold) // -5.0 units
{
    AddReward(config.fallPenalty); // -1.0
    EndEpisode();
}

// Timeout Detection
if (episodeTimer > config.episodeTimeout) // 100 seconds
{
    AddReward(config.fallPenalty); // -1.0
    EndEpisode();
}
```

**Episode Statistics (Logged on End):**
- Total episode reward
- Max distance reached
- Episode length (steps/time)
- Action distribution (jump %, sprint %, roll %, etc.)
- Roll count (if style episode)

---

## Summary

The implementation leverages Unity ML-Agents for efficient parallel training with 28 agents, using PPO with carefully tuned hyperparameters. The dual configuration system separates training parameters from gameplay parameters, enabling flexible experimentation. The 40% style episode frequency represents an empirical choice that balances style learning with functional behavior, acknowledging the arbitrariness of this specific value while providing sufficient signal for the agent to learn aesthetic parkour movements.

**Key Implementation Highlights:**
- **Parallel Architecture:** 28 agents collect experiences simultaneously
- **Distributed Rewards:** Per-step rewards in FixedUpdate, action-specific in OnActionReceived
- **Environment Constraints:** Constraints enforced at execution, not masking
- **Episode-Level Style Flags:** Consistent reward structure per episode
- **Dual Configuration:** YAML for training, ScriptableObject for gameplay
- **Comprehensive Logging:** TensorBoard, progress tracking, checkpoint analysis

The implementation provides a robust foundation for training parkour agents with both functional and aesthetic objectives, with detailed workflows and technical precision enabling reproducible experiments and iterative refinement.
